data_tf[order(A,decreasing = T),][1:10,2:ncol(data_tf)]
rowSums(data_tf[order(A,decreasing = T),][1:10,2:ncol(data_tf)])
A <- rowSums(data_tf[,2:ncol(data_tf)])
data_tf[order(A,decreasing = T),]$localidad[1:10]
rowSums(data_tf[order(A,decreasing = T),][1:10,2:ncol(data_tf)])
colSums(data_tf[,2:ncol(data_tf)])
A <- colSums(data_tf[,2:ncol(data_tf)])}
A <- colSums(data_tf[,2:ncol(data_tf)])
a
A
A[,order(A[2,], decreasing = T)]
A[,order(A[2], decreasing = T)]
A[2]
A[2,]
A[,2]
dim(A)
order(A)
A[order(A)]
A[order(A)][1:10]
names(data_tf=)
names(data_tf)
names(data_tf)[2:]
names(data_tf)[2:ncol(names)]
names(data_tf)[2:ncol(data_tf)]
melt(data_tf, id=c(names(data_tf)[2:ncol(data_tf))]
melt(data_tf, id=c(names(data_tf)[2:ncol(data_tf)]))
install.packages("reshape")
library("reshape")
melt(data_tf, id=c(names(data_tf)[2:ncol(data_tf)]))
melt(data_tf[data_tf$localidad==`Tarapaca/Iquique/Iquique`], id=c(names(data_tf)[2:ncol(data_tf)]))
melt(data_tf[data_tf$localidad=="Tarapaca/Iquique/Iquique"], id=c(names(data_tf)[2:ncol(data_tf)]))
data_tf$localidad=="Tarapaca/Iquique/Iquique"]
data_tf[data_tf$localidad=="Tarapaca/Iquique/Iquique"]
melt(data_tf[,data_tf$localidad=="Tarapaca/Iquique/Iquique"], id=c(names(data_tf)[2:ncol(data_tf)]))
melt(data_tf[data_tf$localidad=="Tarapaca/Iquique/Iquique",], id=c(names(data_tf)[2:ncol(data_tf)]))
melt(data_tf[data_tf$localidad=="Tarapaca/Iquique/Iquique",], id="localidad")
Iquique <- melt(data_tf[data_tf$localidad=="Tarapaca/Iquique/Iquique",], id="localidad")
Iquique[order(Iquique$value),]
Iquique[order(Iquique$value, decreasing = T),]
Iquique[order(Iquique$value, decreasing = T),][1:10,]
Iquique[,2:ncol(Iquique)]
library(ggplot2)
ggplot(Iquique[,2:ncol(Iquique)], aes(x=variable, y=value)) +
geom_bar(stat="identity") +
coord_flip() +
ggtitle("Top 10 conceptos en Iquique") +
xlab("Concepto") + ylab("Frecuencia (cantidad)")
Iquique[order(Iquique$value, decreasing = T),][1:10,][,2:ncol(Iquique)]
Iquique <- melt(data_tf[data_tf$localidad=="Tarapaca/Iquique/Iquique",], id="localidad")
Iquique10 <- Iquique[order(Iquique$value, decreasing = T),][1:10,][,2:ncol(Iquique)]
ggplot(Iquique10, aes(x=variable, y=value)) +
geom_bar(stat="identity") +
coord_flip() +
ggtitle("Top 10 conceptos en Iquique") +
xlab("Concepto") + ylab("Frecuencia (cantidad)")
ggplot(Iquique10[order(Iquique10$value, decreasing = T),], aes(x=variable, y=value)) +
geom_bar(stat="identity") +
coord_flip() +
ggtitle("Top 10 conceptos en Iquique") +
xlab("Concepto") + ylab("Frecuencia (cantidad)")
ggplot(Iquique10, aes(x=reorder(variable, value), y=value)) +
geom_bar(stat="identity") +
coord_flip() +
ggtitle("Top 10 conceptos en Iquique") +
xlab("Concepto") + ylab("Frecuencia (cantidad)")
str(data_tf[12,])
setwd("/home/zsamora/Descargas/Primavera2018/Mineria/Proyecto/")
library(readr)
datos <- read_tsv("dsjVoxArticles.tsv")
df = as.data.frame(datos)
library(RCurl)
library(XML)
install.packages("RCurl")
install.packages("XML")
library(RCurl)
library("RCurl")
install.packages(RCurl)
install.packages("RCurl")
install.packages("XML")
install.packages("xml2")
install.packages("libxml-2.0")
install.packages("libxml2")
install.packages("XML")
setwd("/home/zsamora/")
install.packages("XML")
install.packages("XML")
install.packages("RCurl")
install.packages("RCurl")
df[1,8]
txt <- htmlToText(df[1,8])
library(RCurl)
library(XML)
txt <- htmlToText(df[1,8])
txt <- htmlParse(df[1,8])
txt
htmlToText <- function(input, ...) {
###---PACKAGES ---###
require(RCurl)
require(XML)
###--- LOCAL FUNCTIONS ---###
# Determine how to grab html for a single input element
evaluate_input <- function(input) {
# if input is a .html file
if(file.exists(input)) {
char.vec <- readLines(input, warn = FALSE)
return(paste(char.vec, collapse = ""))
}
# if input is html text
if(grepl("</html>", input, fixed = TRUE)) return(input)
# if input is a URL, probably should use a regex here instead?
if(!grepl(" ", input)) {
# downolad SSL certificate in case of https problem
if(!file.exists("cacert.perm")) download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.perm")
return(getURL(input, followlocation = TRUE, cainfo = "cacert.perm"))
}
# return NULL if none of the conditions above apply
return(NULL)
}
# convert HTML to plain text
convert_html_to_text <- function(html) {
doc <- htmlParse(html, asText = TRUE)
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
return(text)
}
# format text vector into one character string
collapse_text <- function(txt) {
return(paste(txt, collapse = " "))
}
###--- MAIN ---###
# STEP 1: Evaluate input
html.list <- lapply(input, evaluate_input)
# STEP 2: Extract text from HTML
text.list <- lapply(html.list, convert_html_to_text)
# STEP 3: Return text
text.vector <- sapply(text.list, collapse_text)
return(text.vector)
}
txt <- htmlToText(df[1,8])
}
}
htmlToText <- function(input, ...) {
###---PACKAGES ---###
require(RCurl)
require(XML)
###--- LOCAL FUNCTIONS ---###
# Determine how to grab html for a single input element
evaluate_input <- function(input) {
# if input is a .html file
if(file.exists(input)) {
char.vec <- readLines(input, warn = FALSE)
return(paste(char.vec, collapse = ""))
}
# if input is html text
if(grepl("</html>", input, fixed = TRUE)) return(input)
# if input is a URL, probably should use a regex here instead?
if(!grepl(" ", input)) {
# downolad SSL certificate in case of https problem
if(!file.exists("cacert.perm")) download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.perm")
return(getURL(input, followlocation = TRUE, cainfo = "cacert.perm"))
}
# return NULL if none of the conditions above apply
return(NULL)
}
# convert HTML to plain text
convert_html_to_text <- function(html) {
doc <- htmlParse(html, asText = TRUE)
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
return(text)
}
# format text vector into one character string
collapse_text <- function(txt) {
return(paste(txt, collapse = " "))
}
###--- MAIN ---###
# STEP 1: Evaluate input
html.list <- lapply(input, evaluate_input)
# STEP 2: Extract text from HTML
text.list <- lapply(html.list, convert_html_to_text)
# STEP 3: Return text
text.vector <- sapply(text.list, collapse_text)
return(text.vector)
z
}
}}
}
doc <- htmlParse(df[1,8], asText = TRUE)
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
text
text[0]
text[,0]
text[0,]
text[0]
text[1]
text[2]
text[3]
text[4]
res <- paste(text, collapse = " "))
res <- paste(text, collapse = " ")
res
for (t in text){
print(paste("The year is", year))
}
for (t in text){
if (t=="\\n")
print(t)
}
for (t in text){
if (t=="\\n")
print(t)
}
for (t in text){
if (t=="\\n")
print(paste(t))
}
for (t in text){}
for (t in text){ print(t)}
for (t in text){
if (t==" \\n")
t <- NULL
}
text
for (t in text){
if (t==" \\n")
[t]<-NULL
}
for (t in text){
if (t==" \\n"){
[t]<-NULL
}
}
text[text[]!=" //n"]
text <- text[text[]!=" //n"]
text
text <- text[text[,]!=" //n"]
text <- text[text[]!=" //n"]
text
text[unlist(!=" \\n")]
text[unlist(x!=" \\n")]
cond <- lapply(text, function(x) x != " \\n")
cond
text[cond]
text[unlist(cond)]
res <- paste(text, collapse = " "))
res <- paste(text, collapse = " ")
res
text <- text[unlist(cond)]
res <- paste(text, collapse = " ")
res
doc <- htmlParse(df[1,8], asText = TRUE)
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
text
res
text
res
text
text <- text[unlist(cond)]
text
res <- paste(text, collapse = "  ")
res
res <- paste(text, collapse=" ")
res
x <- "Â"
install.packages("stringr")
str_replace_all(x, "[[:punct:]]", " ")
gsub("[[:punct:]]", " ", x)
res
gsub("[[:punct:]]", " ", res)
res
gsub("[^[:punct:]^[:alnum:]]", " ", res)
doc <- htmlParse(df[1,8], asText = TRUE)
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
cond <- lapply(text, function(x) x != "  \\n")
text <- text[unlist(cond)]
text
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
text
doc
doc <- htmlParse(df[1,8], asText = TRUE)
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
cond <- lapply(text, function(x) x != "  \\n")
text <- text[unlist(cond)]
res <- paste(text, collapse = " ")
res <- gsub("[^[:punct:]^[:alnum:]]", " ", res)
res
doc <- htmlParse(df[1,8], asText = TRUE)
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
cond <- lapply(text, function(x) x != " \\n")
text <- text[unlist(cond)]
res <- paste(text, collapse = " ")
res <- gsub("[^[:punct:]^[:alnum:]]", " ", res)
res
doc <- htmlParse(df[1,8], asText = TRUE)
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
cond <- lapply(text, function(x) x != " \\n")
text <- text[unlist(cond)]
res <- paste(text, collapse = " ")
res <- gsub("[^[:punct:]^[a-zA-Z0-9]]", " ", res)
res
doc <- htmlParse(df[1,8], asText = TRUE)
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
cond <- lapply(text, function(x) x != " \\n")
text <- text[unlist(cond)]
res <- paste(text, collapse = " ")
res <- gsub("[^[:punct:][^a-zA-Z0-9]]", " ", res)
res
res
doc <- htmlParse(df[1,8], asText = TRUE)
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
cond <- lapply(text, function(x) x != " \\n")
text <- text[unlist(cond)]
res <- paste(text, collapse = " ")
res <- gsub("[^a-zA-Z0-9]", " ", res)
res
doc <- htmlParse(df[1,8], asText = TRUE)
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
cond <- lapply(text, function(x) x != " \\n")
text <- text[unlist(cond)]
res <- paste(text, collapse = " ")
res <- gsub("[^a-zA-Z0-9[:punct:]]", " ", res)
res
doc <- htmlParse(df[1,8], asText = TRUE)
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
cond <- lapply(text, function(x) x != " \\n")
text <- text[unlist(cond)]
res <- paste(text, collapse = " ")
res <- gsub("[^a-zA-Z0-9[:punct:]]", "", res)
res
doc <- htmlParse(df[1,8], asText = TRUE)
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
cond <- lapply(text, function(x) x != " \\n")
text <- text[unlist(cond)]
res <- paste(text, collapse = " ")
res <- gsub("[^a-zA-Z0-9[:punct:]]", " ", res)
res
library(tidytext)
install.packages("tidytext")
install.packages("tidytext")
install.packages("tidytext")
sentiments
library(tidytext)
sentiments
library(wordcloud)
install.packages(wordcloud2)
install.packages(wordcloud)
install.packages("wordcloud2")
library(wordcloud2)
library(tidyverse)
library(tidytext)
library(glue)
library(stringr)
install.packages("tidyverse")
library(tidyverse)
library(tidytext)
library(glue)
library(stringr)
install.packages("tidyvverse")
library(tidyverse)
library(tidytext)
library(glue)
library(stringr)
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
library(tidyverse)
library(tidytext)
library(glue)
library(stringr)
res
fileText <- gsub("\\$", "", res)
tokens <- data_frame(text = fileText) %>% unnest_tokens(word, text)
tokens
tokens %>%
inner_join(get_sentiments("bing")) %>% # pull out only sentiment words
count(sentiment) %>% # count the # of positive & negative words
spread(sentiment, n, fill = 0) %>% # made data wide rather than narrow
mutate(sentiment = positive - negative) # # of positive words - # of negative owrds
library(“tm”)
library(“wordcloud”)
library(“slam”)
library(“topicmodels”)
library(wordcloud)
library(wordcloud2)
res %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
tokens %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
wordcloud2(tokens)
wordcloud2(Res)
wordcloud2(res)
tokens
tokens %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
tokens %>%
count(word) %>%
with(wordcloud2(word, n, max.words = 100))
tokens %>%
count(word) %>%
with(wordcloud2(word, n, 100))
t <- group_by(tokens)
t
tokens
tokens %>% count(word, sort = TRUE)
wordcloud2(tokens)
tokens %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
tokens %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud2(word, n, max.words = 100))
tokens %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud2(word, n))
tokens %>%
count(word) %>%
with(wordcloud2(word, n))
install.packages("wordcloud")
tokens %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud2(word, n, max.words = 100))
tokens %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
library(wordcloud)
tokens %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
444
tokens %>%
inner_join(get_sentiments("bing")) %>% # pull out only sentiment words
count(sentiment) %>% # count the # of positive & negative words
spread(sentiment, n, fill = 0) %>% # made data wide rather than narrow
mutate(sentiment = positive - negative) # # of positive words - # of negative owrds
df[1,8]
res
install.packages("topicmodels")
install.packages("topicmodels")
install.packages("topicmodels")
install.packages("topicmodels")
res
ap_lda <- LDA(res, k = 2, control = list(seed = 1234))
library(topicmodels)
ap_lda <- LDA(res, k = 2, control = list(seed = 1234))
help LDA
library(tm)
install.packages("ggplot2")
install.packages("GGally")
install.packages("dbscan")
install.packages("cluster")
install.packages("seriation")
install.packages("fpc")
dfvar <- df
db <- dbscan::dbscan(dfvar[, c(1, 2)], eps = 0.9, minPts = 5)
View(dfvar)
df <- read.table("http://users.dcc.uchile.cl/~jherrera/mineria/datasets/d31.txt")
set.seed(2)
wss <- 0
clust = 15 # graficaremos hasta 15 clusters
for (i in 1:clust){
wss[i] <-
sum(kmeans(df, centers=i)$withinss)
}
plot(1:clust, wss, type="b", xlab="Numero de clusters", ylab="wss")
set.seed(2)
km.out <- kmeans(df, 3, nstart = 20)
plot(df, col=(km.out$cluster), main="Resultados usando k = 3 (plot con colores)", xlab="", ylab="", pch=20, cex=2)
kmv2.out <- kmeans(df, 5, nstart = 20)
plot(df, col=(kmv2.out$cluster), main="Resultados usando k = 5 (plot con colores)", xlab="", ylab="", pch=20, cex=2)
library("dbscan")
library("ggplot2")
dfvar <- df
db <- dbscan::dbscan(dfvar[, c(1, 2)], eps = 0.9, minPts = 5)
dfvar$cluster <- factor(db$cluster)
ggplot(dfvar, aes(x=V1, y=V2, colour=cluster)) + geom_point()
View(dfvar)
View(df)
dfvar2 <- df
db2 <- dbscan::dbscan(dfvar2[, c(1, 2)], eps = 0.65, minPts = 5)
install.packages('arules')
install.packages("shiny")
library(shiny)
runExample("01_hello")
getwd
getwd()
setwd("/home/zsamora/Descargas/labcronobiologia/InteractiveLab/")
runApp("InteractiveLab")
runApp("/home/zsamora/Descargas/labcronobiologia/InteractiveLab/")
runApp("/home/zsamora/Descargas/labcronobiologia/InteractiveLab/")
runApp("/home/zsamora/Descargas/labcronobiologia/InteractiveLab/", display.mode = "showcase")
runExample("02_text")
runExample("03_reactivity")
runExample("04_mpg")
runExample("04_sliders")
runExample("05_sliders")
runExample("06_tabsets")
runExample("07_widget")
runExample("07_widgets")
runExample("08_html")
runExample("09_upload")
runExample("10_download")
runExample("11_timer")
runApp()
